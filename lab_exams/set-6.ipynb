{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab Exam - Set 6\n",
    "\n",
    "This notebook contains implementations for all questions in Set 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q21_header",
   "metadata": {},
   "source": [
    "## Question 21: Pandas DataFrame - CSV Import with apply() and map() Transformations\n",
    "\n",
    "**Concepts:**\n",
    "- **DataFrame**: 2D labeled data structure in Pandas (like Excel table)\n",
    "- **apply()**: Apply a function along an axis of DataFrame (works on rows/columns)\n",
    "- **map()**: Apply a function element-wise on a Series (works on single column)\n",
    "- **Lambda functions**: Anonymous functions for quick transformations\n",
    "- **Transformations**: Modifying data values (e.g., converting units, categorizing, formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q21_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample CSV file for demonstration\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Age': [25, 30, 35, 28, 32, 22, 29, 31],\n",
    "    'Salary': [50000, 60000, 75000, 55000, 70000, 48000, 62000, 68000],\n",
    "    'Department': ['HR', 'IT', 'Finance', 'IT', 'HR', 'Finance', 'IT', 'HR'],\n",
    "    'Experience': [2, 5, 8, 3, 6, 1, 4, 7]\n",
    "}\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample.to_csv('employee_data.csv', index=False)\n",
    "\n",
    "# Import CSV file\n",
    "df = pd.read_csv('employee_data.csv')\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Transformation 1: Using map() - Convert Department codes\n",
    "# Map department names to department codes\n",
    "dept_mapping = {'HR': 'D01', 'IT': 'D02', 'Finance': 'D03'}\n",
    "df['Dept_Code'] = df['Department'].map(dept_mapping)\n",
    "\n",
    "print(\"Transformation 1 - map() for Department Codes:\")\n",
    "print(df[['Name', 'Department', 'Dept_Code']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Transformation 2: Using map() - Categorize age groups\n",
    "def age_category(age):\n",
    "    if age < 25:\n",
    "        return 'Junior'\n",
    "    elif age < 30:\n",
    "        return 'Mid-Level'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "df['Age_Category'] = df['Age'].map(age_category)\n",
    "\n",
    "print(\"Transformation 2 - map() for Age Categories:\")\n",
    "print(df[['Name', 'Age', 'Age_Category']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Transformation 3: Using apply() - Calculate bonus (10% of salary)\n",
    "df['Bonus'] = df['Salary'].apply(lambda x: x * 0.10)\n",
    "\n",
    "print(\"Transformation 3 - apply() for Bonus Calculation:\")\n",
    "print(df[['Name', 'Salary', 'Bonus']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Transformation 4: Using apply() on multiple columns - Calculate total compensation\n",
    "df['Total_Compensation'] = df.apply(lambda row: row['Salary'] + row['Bonus'] + (row['Experience'] * 1000), axis=1)\n",
    "\n",
    "print(\"Transformation 4 - apply() for Total Compensation (Salary + Bonus + Experience*1000):\")\n",
    "print(df[['Name', 'Salary', 'Bonus', 'Experience', 'Total_Compensation']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Transformation 5: Using apply() - Format name to uppercase\n",
    "df['Name_Upper'] = df['Name'].apply(str.upper)\n",
    "\n",
    "# Transformation 6: Using map() with lambda - Salary in thousands\n",
    "df['Salary_K'] = df['Salary'].map(lambda x: f\"{x/1000:.1f}K\")\n",
    "\n",
    "print(\"Transformation 5 & 6 - apply() and map() for Formatting:\")\n",
    "print(df[['Name', 'Name_Upper', 'Salary', 'Salary_K']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Final DataFrame with all transformations\n",
    "print(\"Final DataFrame with All Transformations:\")\n",
    "print(df)\n",
    "\n",
    "# Summary of transformations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY OF TRANSFORMATIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. map() - Converted Department to Dept_Code\")\n",
    "print(\"2. map() - Categorized Age into Age_Category\")\n",
    "print(\"3. apply() - Calculated Bonus (10% of Salary)\")\n",
    "print(\"4. apply() - Calculated Total_Compensation using multiple columns\")\n",
    "print(\"5. apply() - Converted Name to uppercase\")\n",
    "print(\"6. map() - Formatted Salary in thousands (K)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q22_header",
   "metadata": {},
   "source": [
    "## Question 22: Data Preprocessing - Missing Data, Outliers, and Standardization\n",
    "\n",
    "**Concepts:**\n",
    "- **Missing data**: Empty or null entries (handled with `fillna()`, `dropna()`, imputation)\n",
    "- **Outliers**: Data points significantly different from others (detected using IQR, Z-score)\n",
    "- **IQR (Interquartile Range)**: Q3 - Q1, used to find outliers (values beyond Q1-1.5*IQR or Q3+1.5*IQR)\n",
    "- **Standardization**: Scaling features to have mean=0 and std=1 using formula: (x - mean) / std\n",
    "- **StandardScaler**: Sklearn tool for standardization (important for algorithms sensitive to scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q22_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a sample dataset with missing values and outliers\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'ID': range(1, 21),\n",
    "    'Age': [25, 30, np.nan, 28, 35, 22, 29, np.nan, 31, 27, 26, 33, 150, 29, 28, 30, np.nan, 32, 29, 31],\n",
    "    'Salary': [50000, 60000, 55000, np.nan, 70000, 48000, 62000, 58000, 200000, 54000, \n",
    "               52000, 68000, 61000, np.nan, 57000, 63000, 59000, 56000, 300000, 62000],\n",
    "    'Score': [85, 90, 78, 88, np.nan, 92, 87, 89, 91, 10, 86, 88, 90, 87, np.nan, 89, 91, 88, 86, 90],\n",
    "    'Experience': [2, 5, 3, 3, np.nan, 1, 4, 4, 6, 2, 2, 7, 8, 4, 3, 5, 4, 6, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "print(\"STEP 1: HANDLING MISSING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values count:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(df.isnull(), cmap='viridis', cbar=True, yticklabels=False)\n",
    "plt.title('Missing Data Visualization (Yellow = Missing)')\n",
    "plt.show()\n",
    "\n",
    "# Fill missing values with different strategies\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)  # Median for Age\n",
    "df['Salary'].fillna(df['Salary'].mean(), inplace=True)  # Mean for Salary\n",
    "df['Score'].fillna(df['Score'].mean(), inplace=True)  # Mean for Score\n",
    "df['Experience'].fillna(df['Experience'].mode()[0], inplace=True)  # Mode for Experience\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 2: Detect Outliers using IQR method\n",
    "print(\"STEP 2: DETECTING OUTLIERS (IQR Method)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range (IQR) method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    outlier_indices = outliers.index.tolist()\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR, outlier_indices\n",
    "\n",
    "# Detect outliers for numerical columns\n",
    "numeric_cols = ['Age', 'Salary', 'Score', 'Experience']\n",
    "outlier_info = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper, Q1, Q3, IQR, outlier_idx = detect_outliers_iqr(df, col)\n",
    "    outlier_info[col] = outlier_idx\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Q1 (25th percentile): {Q1:.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): {Q3:.2f}\")\n",
    "    print(f\"  IQR: {IQR:.2f}\")\n",
    "    print(f\"  Lower Bound: {lower:.2f}\")\n",
    "    print(f\"  Upper Bound: {upper:.2f}\")\n",
    "    \n",
    "    if not outliers.empty:\n",
    "        print(f\"  Outliers found ({len(outliers)}):\")\n",
    "        for idx, row in outliers.iterrows():\n",
    "            print(f\"    ID {row['ID']}: {col} = {row[col]:.2f}\")\n",
    "    else:\n",
    "        print(\"  No outliers found\")\n",
    "\n",
    "# Visualize outliers using box plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Outlier Detection using Box Plots', fontsize=16)\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    row = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    \n",
    "    axes[row, col_idx].boxplot(df[col], vert=True)\n",
    "    axes[row, col_idx].set_title(f'{col} - Box Plot')\n",
    "    axes[row, col_idx].set_ylabel(col)\n",
    "    axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark outliers\n",
    "    if outlier_info[col]:\n",
    "        outlier_values = df.loc[outlier_info[col], col]\n",
    "        axes[row, col_idx].scatter([1]*len(outlier_values), outlier_values, \n",
    "                                   color='red', s=100, zorder=3, label='Outliers')\n",
    "        axes[row, col_idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 3: Handle outliers (remove them for this example)\n",
    "print(\"STEP 3: HANDLING OUTLIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all unique outlier indices\n",
    "all_outlier_indices = set()\n",
    "for indices in outlier_info.values():\n",
    "    all_outlier_indices.update(indices)\n",
    "\n",
    "print(f\"\\nTotal rows with outliers: {len(all_outlier_indices)}\")\n",
    "print(f\"Outlier row indices: {sorted(all_outlier_indices)}\")\n",
    "\n",
    "# Remove outliers\n",
    "df_clean = df.drop(index=all_outlier_indices).reset_index(drop=True)\n",
    "print(f\"\\nDataFrame shape before: {df.shape}\")\n",
    "print(f\"DataFrame shape after removing outliers: {df_clean.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Step 4: Standardization (Feature Scaling)\n",
    "print(\"STEP 4: STANDARDIZATION (FEATURE SCALING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Statistics before standardization\n",
    "print(\"\\nStatistics BEFORE standardization:\")\n",
    "print(df_clean[numeric_cols].describe())\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_clean.copy()\n",
    "df_scaled[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "print(\"\\nStatistics AFTER standardization:\")\n",
    "print(df_scaled[numeric_cols].describe())\n",
    "\n",
    "# Verify mean ≈ 0 and std ≈ 1\n",
    "print(\"\\nVerification (Mean should be ~0, Std should be ~1):\")\n",
    "for col in numeric_cols:\n",
    "    mean = df_scaled[col].mean()\n",
    "    std = df_scaled[col].std()\n",
    "    print(f\"{col}: Mean = {mean:.6f}, Std = {std:.6f}\")\n",
    "\n",
    "# Visualize before and after standardization\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Before and After Standardization', fontsize=16)\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    # Before standardization\n",
    "    axes[0, idx].hist(df_clean[col], bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, idx].set_title(f'{col} (Original)')\n",
    "    axes[0, idx].set_xlabel('Value')\n",
    "    axes[0, idx].set_ylabel('Frequency')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # After standardization\n",
    "    axes[1, idx].hist(df_scaled[col], bins=10, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    axes[1, idx].set_title(f'{col} (Standardized)')\n",
    "    axes[1, idx].set_xlabel('Standardized Value')\n",
    "    axes[1, idx].set_ylabel('Frequency')\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOriginal Data (first 5 rows):\")\n",
    "print(df_clean[numeric_cols].head())\n",
    "print(\"\\nStandardized Data (first 5 rows):\")\n",
    "print(df_scaled[numeric_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q23_header",
   "metadata": {},
   "source": [
    "## Question 23: Data Visualization - Line Plot and Scatter Plot with Regression Line\n",
    "\n",
    "**Concepts:**\n",
    "- **Line plot**: Graph showing trends over continuous data (useful for time series, sequential data)\n",
    "- **Scatter plot**: Graph showing relationship between two variables\n",
    "- **Regression line**: Best-fit line showing linear relationship between variables\n",
    "- **Linear regression**: Statistical method to model relationship: y = mx + c\n",
    "- **Correlation**: Measure of how strongly variables are related (-1 to +1)\n",
    "- **Matplotlib/Seaborn**: Python libraries for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q23_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "# Create a sample dataset showing relationship between variables\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Generate data: Experience vs Salary (positive correlation)\n",
    "experience = np.linspace(0, 15, n_samples)  # Years of experience (0-15)\n",
    "# Salary increases with experience, with some random noise\n",
    "salary = 30000 + (experience * 4000) + np.random.normal(0, 5000, n_samples)\n",
    "\n",
    "# Generate data: Study Hours vs Exam Score (positive correlation)\n",
    "study_hours = np.linspace(1, 10, n_samples)  # Study hours per day\n",
    "exam_score = 40 + (study_hours * 5) + np.random.normal(0, 5, n_samples)\n",
    "exam_score = np.clip(exam_score, 0, 100)  # Keep scores between 0-100\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Experience': experience,\n",
    "    'Salary': salary,\n",
    "    'Study_Hours': study_hours,\n",
    "    'Exam_Score': exam_score\n",
    "})\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Calculate correlations\n",
    "corr1 = df['Experience'].corr(df['Salary'])\n",
    "corr2 = df['Study_Hours'].corr(df['Exam_Score'])\n",
    "print(f\"Correlation between Experience and Salary: {corr1:.4f}\")\n",
    "print(f\"Correlation between Study Hours and Exam Score: {corr2:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# PLOT 1: Line Plot - Trend of Salary over Experience\n",
    "print(\"Creating Line Plot: Experience vs Salary Trend...\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Line plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['Experience'], df['Salary'], linewidth=2, color='blue', marker='o', \n",
    "         markersize=4, alpha=0.6, label='Salary Trend')\n",
    "plt.xlabel('Years of Experience', fontsize=12)\n",
    "plt.ylabel('Salary ($)', fontsize=12)\n",
    "plt.title('Line Plot: Salary Trend over Experience', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Line plot with moving average\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calculate moving average for smoother trend\n",
    "window = 10\n",
    "df['Salary_MA'] = df['Salary'].rolling(window=window).mean()\n",
    "\n",
    "plt.plot(df['Experience'], df['Salary'], alpha=0.3, color='gray', label='Raw Data')\n",
    "plt.plot(df['Experience'], df['Salary_MA'], linewidth=3, color='red', \n",
    "         label=f'{window}-point Moving Average')\n",
    "plt.xlabel('Years of Experience', fontsize=12)\n",
    "plt.ylabel('Salary ($)', fontsize=12)\n",
    "plt.title('Line Plot: Salary with Moving Average', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# PLOT 2: Scatter Plot with Regression Line\n",
    "print(\"Creating Scatter Plots with Regression Lines...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Scatter Plots with Regression Lines', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Scatter Plot 1: Experience vs Salary\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Create scatter plot\n",
    "ax1.scatter(df['Experience'], df['Salary'], alpha=0.6, s=50, color='blue', \n",
    "            edgecolors='black', linewidth=0.5, label='Data Points')\n",
    "\n",
    "# Calculate regression line using sklearn\n",
    "X1 = df['Experience'].values.reshape(-1, 1)\n",
    "y1 = df['Salary'].values\n",
    "reg1 = LinearRegression()\n",
    "reg1.fit(X1, y1)\n",
    "y1_pred = reg1.predict(X1)\n",
    "\n",
    "# Plot regression line\n",
    "ax1.plot(df['Experience'], y1_pred, color='red', linewidth=3, \n",
    "         label=f'Regression Line\\ny = {reg1.coef_[0]:.2f}x + {reg1.intercept_:.2f}')\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared1 = reg1.score(X1, y1)\n",
    "\n",
    "ax1.set_xlabel('Years of Experience', fontsize=12)\n",
    "ax1.set_ylabel('Salary ($)', fontsize=12)\n",
    "ax1.set_title(f'Experience vs Salary\\n(R² = {r_squared1:.4f}, Correlation = {corr1:.4f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Scatter Plot 2: Study Hours vs Exam Score\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Create scatter plot\n",
    "ax2.scatter(df['Study_Hours'], df['Exam_Score'], alpha=0.6, s=50, color='green', \n",
    "            edgecolors='black', linewidth=0.5, label='Data Points')\n",
    "\n",
    "# Calculate regression line\n",
    "X2 = df['Study_Hours'].values.reshape(-1, 1)\n",
    "y2 = df['Exam_Score'].values\n",
    "reg2 = LinearRegression()\n",
    "reg2.fit(X2, y2)\n",
    "y2_pred = reg2.predict(X2)\n",
    "\n",
    "# Plot regression line\n",
    "ax2.plot(df['Study_Hours'], y2_pred, color='red', linewidth=3, \n",
    "         label=f'Regression Line\\ny = {reg2.coef_[0]:.2f}x + {reg2.intercept_:.2f}')\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared2 = reg2.score(X2, y2)\n",
    "\n",
    "ax2.set_xlabel('Study Hours per Day', fontsize=12)\n",
    "ax2.set_ylabel('Exam Score', fontsize=12)\n",
    "ax2.set_title(f'Study Hours vs Exam Score\\n(R² = {r_squared2:.4f}, Correlation = {corr2:.4f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional: Combined visualization using seaborn\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Creating enhanced visualization with confidence intervals...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Scatter Plots with Regression Lines and Confidence Intervals (Seaborn)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Using seaborn's regplot for better visualization with confidence intervals\n",
    "sns.regplot(x='Experience', y='Salary', data=df, ax=axes[0], \n",
    "            scatter_kws={'alpha':0.6, 's':50, 'edgecolors':'black', 'linewidth':0.5},\n",
    "            line_kws={'color':'red', 'linewidth':3})\n",
    "axes[0].set_title(f'Experience vs Salary\\n(R² = {r_squared1:.4f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Years of Experience', fontsize=12)\n",
    "axes[0].set_ylabel('Salary ($)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "sns.regplot(x='Study_Hours', y='Exam_Score', data=df, ax=axes[1],\n",
    "            scatter_kws={'alpha':0.6, 's':50, 'edgecolors':'black', 'linewidth':0.5, 'color':'green'},\n",
    "            line_kws={'color':'red', 'linewidth':3})\n",
    "axes[1].set_title(f'Study Hours vs Exam Score\\n(R² = {r_squared2:.4f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Study Hours per Day', fontsize=12)\n",
    "axes[1].set_ylabel('Exam Score', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print regression statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGRESSION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Experience vs Salary:\")\n",
    "print(f\"   - Regression Equation: Salary = {reg1.coef_[0]:.2f} × Experience + {reg1.intercept_:.2f}\")\n",
    "print(f\"   - Slope (coefficient): {reg1.coef_[0]:.2f} (salary increases by ${reg1.coef_[0]:.2f} per year)\")\n",
    "print(f\"   - Intercept: ${reg1.intercept_:.2f} (starting salary)\")\n",
    "print(f\"   - R-squared: {r_squared1:.4f} ({r_squared1*100:.2f}% variance explained)\")\n",
    "print(f\"   - Correlation: {corr1:.4f}\")\n",
    "\n",
    "print(\"\\n2. Study Hours vs Exam Score:\")\n",
    "print(f\"   - Regression Equation: Score = {reg2.coef_[0]:.2f} × Study_Hours + {reg2.intercept_:.2f}\")\n",
    "print(f\"   - Slope (coefficient): {reg2.coef_[0]:.2f} (score increases by {reg2.coef_[0]:.2f} per hour)\")\n",
    "print(f\"   - Intercept: {reg2.intercept_:.2f} (base score)\")\n",
    "print(f\"   - R-squared: {r_squared2:.4f} ({r_squared2*100:.2f}% variance explained)\")\n",
    "print(f\"   - Correlation: {corr2:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n- Both relationships show strong positive correlations (values close to 1)\")\n",
    "print(\"- R² values indicate how well the regression line fits the data\")\n",
    "print(\"- Higher R² means the independent variable better explains the dependent variable\")\n",
    "print(\"- The regression line helps predict values and identify trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q24_header",
   "metadata": {},
   "source": [
    "## Question 24: K-Nearest Neighbors (KNN) Classifier - Compare Different k-values\n",
    "\n",
    "**Concepts:**\n",
    "- **KNN**: Supervised learning algorithm that classifies based on k nearest neighbors\n",
    "- **k-value**: Number of nearest neighbors to consider (hyperparameter)\n",
    "- **How KNN works**: Finds k closest points, uses majority vote for classification\n",
    "- **Distance metric**: Usually Euclidean distance to find nearest neighbors\n",
    "- **Choosing k**: Small k = more complex (overfitting), Large k = simpler (underfitting)\n",
    "- **Train-test split**: Divide data to train model and evaluate performance\n",
    "- **Accuracy**: Percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q24_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "print(\"Loading Iris Dataset...\")\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = y\n",
    "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['species_name'].value_counts())\n",
    "\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Split dataset into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                      random_state=42, stratify=y)\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Feature scaling (important for KNN as it uses distance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling applied (StandardScaler)\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test different k-values\n",
    "k_values = [1, 3, 5, 7, 9, 11, 15, 19, 25, 31]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Training KNN Classifiers with different k-values...\\n\")\n",
    "print(f\"{'k-value':<10} {'Train Accuracy':<20} {'Test Accuracy':<20}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Store models for later use\n",
    "models = {}\n",
    "\n",
    "for k in k_values:\n",
    "    # Create and train KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on both train and test sets\n",
    "    y_train_pred = knn.predict(X_train_scaled)\n",
    "    y_test_pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    models[k] = knn\n",
    "    \n",
    "    print(f\"{k:<10} {train_acc*100:<20.2f} {test_acc*100:<20.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Find best k-value\n",
    "best_k_idx = np.argmax(test_accuracies)\n",
    "best_k = k_values[best_k_idx]\n",
    "best_accuracy = test_accuracies[best_k_idx]\n",
    "\n",
    "print(f\"Best k-value: {best_k}\")\n",
    "print(f\"Best test accuracy: {best_accuracy*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Plot 1: Accuracy comparison for different k-values\n",
    "print(\"Creating accuracy comparison plots...\")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Line plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, train_accuracies, marker='o', linewidth=2, markersize=8, \n",
    "         label='Training Accuracy', color='blue')\n",
    "plt.plot(k_values, test_accuracies, marker='s', linewidth=2, markersize=8, \n",
    "         label='Testing Accuracy', color='red')\n",
    "plt.axvline(x=best_k, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Best k={best_k}')\n",
    "plt.xlabel('k-value (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('KNN Accuracy vs k-value', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "\n",
    "# Subplot 2: Bar plot\n",
    "plt.subplot(1, 2, 2)\n",
    "x_pos = np.arange(len(k_values))\n",
    "width = 0.35\n",
    "plt.bar(x_pos - width/2, train_accuracies, width, label='Training Accuracy', \n",
    "        color='blue', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, test_accuracies, width, label='Testing Accuracy', \n",
    "        color='red', alpha=0.7)\n",
    "plt.xlabel('k-value', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('KNN Accuracy Comparison (Bar Plot)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x_pos, k_values)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Detailed accuracy table visualization\n",
    "print(\"\\nCreating detailed comparison visualization...\")\n",
    "\n",
    "# Create DataFrame for results\n",
    "results_df = pd.DataFrame({\n",
    "    'k-value': k_values,\n",
    "    'Train_Accuracy': [f\"{acc*100:.2f}%\" for acc in train_accuracies],\n",
    "    'Test_Accuracy': [f\"{acc*100:.2f}%\" for acc in test_accuracies],\n",
    "    'Train_Acc_Numeric': train_accuracies,\n",
    "    'Test_Acc_Numeric': test_accuracies\n",
    "})\n",
    "\n",
    "print(\"\\nAccuracy Results Table:\")\n",
    "print(results_df[['k-value', 'Train_Accuracy', 'Test_Accuracy']])\n",
    "\n",
    "# Heatmap visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of accuracies\n",
    "heatmap_data = np.array([train_accuracies, test_accuracies])\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "            xticklabels=k_values, yticklabels=['Train', 'Test'],\n",
    "            cbar_kws={'label': 'Accuracy'}, ax=axes[0])\n",
    "axes[0].set_title('Accuracy Heatmap for Different k-values', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('k-value', fontsize=12)\n",
    "\n",
    "# Difference between train and test accuracy\n",
    "accuracy_diff = np.array(train_accuracies) - np.array(test_accuracies)\n",
    "axes[1].bar(k_values, accuracy_diff, color=['red' if d > 0.05 else 'green' for d in accuracy_diff],\n",
    "            alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1].axhline(y=0.05, color='orange', linestyle='--', linewidth=1, label='Overfitting threshold')\n",
    "axes[1].set_xlabel('k-value', fontsize=12)\n",
    "axes[1].set_ylabel('Train Accuracy - Test Accuracy', fontsize=12)\n",
    "axes[1].set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Detailed analysis of best model\n",
    "print(f\"DETAILED ANALYSIS OF BEST MODEL (k={best_k})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = models[best_k]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=iris.target_names))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - KNN Classifier (k={best_k})\\nAccuracy: {best_accuracy*100:.2f}%', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Best performing k-value: {best_k} with test accuracy of {best_accuracy*100:.2f}%\")\n",
    "print(f\"\\n2. Overfitting Analysis:\")\n",
    "for i, k in enumerate(k_values):\n",
    "    diff = train_accuracies[i] - test_accuracies[i]\n",
    "    if diff > 0.05:\n",
    "        print(f\"   - k={k}: Potential overfitting (difference: {diff*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n3. General Observations:\")\n",
    "print(f\"   - Very small k (e.g., k=1): May overfit, sensitive to noise\")\n",
    "print(f\"   - Very large k (e.g., k=31): May underfit, too simplified\")\n",
    "print(f\"   - Moderate k values typically perform best\")\n",
    "print(f\"   - Odd k values preferred for binary classification to avoid ties\")\n",
    "\n",
    "print(f\"\\n4. Recommendation:\")\n",
    "print(f\"   - Use k={best_k} for this dataset\")\n",
    "print(f\"   - Always perform cross-validation for robust k selection\")\n",
    "print(f\"   - Consider feature scaling (already applied here)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
