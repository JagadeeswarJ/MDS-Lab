{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab Exam - Set 8\n",
    "\n",
    "This notebook contains implementations for all questions in Set 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## Question 29: Pandas DataFrame - Advanced String Manipulations\n",
    "\n",
    "**Concepts:**\n",
    "- **String operations**: Methods to manipulate text data in DataFrames\n",
    "- **Case conversion**: Converting text to upper/lower case using `.str.upper()`, `.str.lower()`\n",
    "- **String splitting**: Breaking strings into parts using `.str.split()`\n",
    "- **String replacement**: Replacing substrings using `.str.replace()`\n",
    "- **Pattern matching**: Finding patterns using `.str.contains()`, `.str.extract()`\n",
    "- **String slicing**: Extracting portions of strings using `.str[]` or `.str.slice()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with textual columns\n",
    "data = {\n",
    "    'Name': ['John Doe', 'jane smith', 'ALICE JOHNSON', 'Bob Brown', 'charlie DAVIS'],\n",
    "    'Email': ['john.doe@email.com', 'jane.smith@gmail.com', 'alice@yahoo.com', \n",
    "              'bob.brown@email.com', 'charlie@hotmail.com'],\n",
    "    'Phone': ['+1-555-1234', '+1-555-5678', '+1-555-9012', '+1-555-3456', '+1-555-7890'],\n",
    "    'Address': ['123 Main St, New York, NY', '456 Oak Ave, Los Angeles, CA', \n",
    "                '789 Pine Rd, Chicago, IL', '321 Elm St, Houston, TX', \n",
    "                '654 Maple Dr, Phoenix, AZ'],\n",
    "    'Product_Code': ['PROD-001-A', 'PROD-002-B', 'PROD-003-C', 'PROD-004-A', 'PROD-005-B']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 1. Case conversion - Standardize name format to Title Case\n",
    "df['Name_Standardized'] = df['Name'].str.title()\n",
    "print(\"1. Standardized Names (Title Case):\")\n",
    "print(df[['Name', 'Name_Standardized']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. Extract first and last names by splitting\n",
    "df[['First_Name', 'Last_Name']] = df['Name_Standardized'].str.split(' ', n=1, expand=True)\n",
    "print(\"2. Split Names into First and Last:\")\n",
    "print(df[['Name_Standardized', 'First_Name', 'Last_Name']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 3. Extract domain from email addresses\n",
    "df['Email_Domain'] = df['Email'].str.extract(r'@([\\w.]+)')\n",
    "print(\"3. Extract Email Domain:\")\n",
    "print(df[['Email', 'Email_Domain']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Clean phone numbers - remove special characters\n",
    "df['Phone_Cleaned'] = df['Phone'].str.replace(r'[^0-9]', '', regex=True)\n",
    "print(\"4. Cleaned Phone Numbers (digits only):\")\n",
    "print(df[['Phone', 'Phone_Cleaned']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 5. Extract city from address (second element after splitting by comma)\n",
    "df['City'] = df['Address'].str.split(',').str[1].str.strip()\n",
    "print(\"5. Extract City from Address:\")\n",
    "print(df[['Address', 'City']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 6. Extract state abbreviation from address (last part)\n",
    "df['State'] = df['Address'].str.split(',').str[-1].str.strip()\n",
    "print(\"6. Extract State from Address:\")\n",
    "print(df[['Address', 'State']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 7. Check if email contains specific domain\n",
    "df['Is_Gmail'] = df['Email'].str.contains('gmail', case=False)\n",
    "print(\"7. Check if Email is Gmail:\")\n",
    "print(df[['Email', 'Is_Gmail']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. Extract product category (last character after last hyphen)\n",
    "df['Product_Category'] = df['Product_Code'].str.split('-').str[-1]\n",
    "print(\"8. Extract Product Category:\")\n",
    "print(df[['Product_Code', 'Product_Category']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 9. Create initials from names\n",
    "df['Initials'] = df['First_Name'].str[0] + df['Last_Name'].str[0]\n",
    "print(\"9. Create Initials:\")\n",
    "print(df[['Name_Standardized', 'Initials']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 10. String length calculation\n",
    "df['Name_Length'] = df['Name_Standardized'].str.len()\n",
    "print(\"10. Calculate Name Length:\")\n",
    "print(df[['Name_Standardized', 'Name_Length']])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Display final DataFrame with selected columns\n",
    "print(\"Final DataFrame with String Manipulations:\")\n",
    "display_cols = ['Name_Standardized', 'First_Name', 'Last_Name', 'Email_Domain', \n",
    "                'Phone_Cleaned', 'City', 'State', 'Product_Category']\n",
    "print(df[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "## Question 30: Missing Data Imputation and Feature Scaling\n",
    "\n",
    "**Concepts:**\n",
    "- **Missing data**: Incomplete values in dataset (NaN or None)\n",
    "- **Imputation**: Filling missing values with substitutes (mean, median, mode)\n",
    "- **Feature scaling**: Transforming features to similar ranges\n",
    "- **Standardization**: Scaling data to mean=0 and std=1 using StandardScaler\n",
    "- **Normalization**: Scaling data to range [0,1] using MinMaxScaler\n",
    "- **SimpleImputer**: Scikit-learn tool for filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a dataset with missing values\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Age': [25, 30, np.nan, 28, 35, np.nan, 22, 29, 31, 27, np.nan, 33],\n",
    "    'Salary': [50000, 60000, 55000, np.nan, 70000, 48000, np.nan, 62000, 58000, 51000, 65000, np.nan],\n",
    "    'Experience': [2, 5, 3, np.nan, 8, 1, 2, np.nan, 6, 3, 7, 9],\n",
    "    'Score': [85, 90, np.nan, 88, 92, 78, 83, np.nan, 89, 86, np.nan, 91],\n",
    "    'Rating': [4.2, np.nan, 3.8, 4.5, 4.8, 3.5, np.nan, 4.3, 4.6, 4.0, 4.7, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame with Missing Values:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Missing percentage: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Visualize missing data pattern\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Data Pattern (Yellow = Missing)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== IMPUTATION ====================\n",
    "\n",
    "# Method 1: Mean imputation for Age and Experience\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "df[['Age', 'Experience']] = mean_imputer.fit_transform(df[['Age', 'Experience']])\n",
    "\n",
    "# Method 2: Median imputation for Salary\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "df[['Salary']] = median_imputer.fit_transform(df[['Salary']])\n",
    "\n",
    "# Method 3: Most frequent (mode) imputation for Score\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['Score']] = mode_imputer.fit_transform(df[['Score']])\n",
    "\n",
    "# Method 4: Constant imputation for Rating (fill with median)\n",
    "rating_median = df['Rating'].median()\n",
    "df['Rating'].fillna(rating_median, inplace=True)\n",
    "\n",
    "print(\"DataFrame After Imputation:\")\n",
    "print(df)\n",
    "print(f\"\\nMissing values remaining: {df.isnull().sum().sum()}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ==================== FEATURE SCALING ====================\n",
    "\n",
    "# Create a copy for scaling demonstration\n",
    "df_original = df.copy()\n",
    "\n",
    "# 1. Standardization (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_standardized = df.copy()\n",
    "df_standardized[df.columns] = scaler_standard.fit_transform(df)\n",
    "\n",
    "print(\"Standardized Data (Mean=0, Std=1):\")\n",
    "print(df_standardized.head())\n",
    "print(\"\\nStatistics after Standardization:\")\n",
    "print(df_standardized.describe())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# 2. Min-Max Normalization (scaling to [0,1])\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_normalized = df.copy()\n",
    "df_normalized[df.columns] = scaler_minmax.fit_transform(df)\n",
    "\n",
    "print(\"Normalized Data (Range [0,1]):\")\n",
    "print(df_normalized.head())\n",
    "print(\"\\nStatistics after Normalization:\")\n",
    "print(df_normalized.describe())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Visualize the effects of scaling\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 10))\n",
    "fig.suptitle('Comparison: Original vs Standardized vs Normalized Data', fontsize=16)\n",
    "\n",
    "columns = df.columns\n",
    "for idx, col in enumerate(columns):\n",
    "    # Original data\n",
    "    axes[0, idx].hist(df_original[col], bins=10, color='skyblue', edgecolor='black')\n",
    "    axes[0, idx].set_title(f'{col} (Original)')\n",
    "    axes[0, idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Standardized data\n",
    "    axes[1, idx].hist(df_standardized[col], bins=10, color='lightcoral', edgecolor='black')\n",
    "    axes[1, idx].set_title(f'{col} (Standardized)')\n",
    "    axes[1, idx].set_ylabel('Frequency')\n",
    "    \n",
    "    # Normalized data\n",
    "    axes[2, idx].hist(df_normalized[col], bins=10, color='lightgreen', edgecolor='black')\n",
    "    axes[2, idx].set_title(f'{col} (Normalized)')\n",
    "    axes[2, idx].set_ylabel('Frequency')\n",
    "    axes[2, idx].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare statistics\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"\\nOriginal Data - Mean and Std:\")\n",
    "print(pd.DataFrame({'Mean': df_original.mean(), 'Std': df_original.std()}))\n",
    "print(\"\\nStandardized Data - Mean and Std:\")\n",
    "print(pd.DataFrame({'Mean': df_standardized.mean(), 'Std': df_standardized.std()}))\n",
    "print(\"\\nNormalized Data - Min and Max:\")\n",
    "print(pd.DataFrame({'Min': df_normalized.min(), 'Max': df_normalized.max()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1v2w3x4",
   "metadata": {},
   "source": [
    "## Question 31: Scatter Plots and Line Charts for Feature Visualization\n",
    "\n",
    "**Concepts:**\n",
    "- **Scatter plot**: Shows relationship between two continuous variables as points\n",
    "- **Line chart**: Displays data points connected by lines, good for trends over time\n",
    "- **Correlation**: Measure of how two variables are related (-1 to +1)\n",
    "- **Trend analysis**: Identifying patterns in data over time or across variables\n",
    "- **Matplotlib/Seaborn**: Python libraries for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Create a comprehensive dataset with numerical features\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "data = {\n",
    "    'Hours_Studied': np.random.uniform(1, 10, n_samples),\n",
    "    'Test_Score': None,\n",
    "    'Age': np.random.randint(18, 30, n_samples),\n",
    "    'Sleep_Hours': np.random.uniform(4, 9, n_samples),\n",
    "    'Income': np.random.uniform(30000, 100000, n_samples),\n",
    "    'Experience_Years': np.random.uniform(0, 10, n_samples),\n",
    "    'Month': np.tile(range(1, 13), n_samples // 12 + 1)[:n_samples]\n",
    "}\n",
    "\n",
    "# Create correlations: Test_Score depends on Hours_Studied and Sleep_Hours\n",
    "data['Test_Score'] = (data['Hours_Studied'] * 5 + \n",
    "                      data['Sleep_Hours'] * 3 + \n",
    "                      np.random.normal(0, 5, n_samples) + 40)\n",
    "data['Test_Score'] = np.clip(data['Test_Score'], 0, 100)\n",
    "\n",
    "# Income depends on Experience_Years and Age\n",
    "data['Income'] = (data['Experience_Years'] * 5000 + \n",
    "                  data['Age'] * 1000 + \n",
    "                  np.random.normal(0, 5000, n_samples) + 30000)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df[['Hours_Studied', 'Test_Score', 'Age', 'Sleep_Hours', \n",
    "                          'Income', 'Experience_Years']].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ==================== SCATTER PLOTS ====================\n",
    "\n",
    "# 1. Basic scatter plots showing relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Scatter Plots - Relationship Between Numerical Features', fontsize=16)\n",
    "\n",
    "# Scatter 1: Hours Studied vs Test Score\n",
    "axes[0, 0].scatter(df['Hours_Studied'], df['Test_Score'], alpha=0.6, color='blue')\n",
    "axes[0, 0].set_xlabel('Hours Studied')\n",
    "axes[0, 0].set_ylabel('Test Score')\n",
    "axes[0, 0].set_title('Hours Studied vs Test Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "# Add trend line\n",
    "z = np.polyfit(df['Hours_Studied'], df['Test_Score'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(df['Hours_Studied'], p(df['Hours_Studied']), \"r--\", alpha=0.8, label='Trend')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Scatter 2: Sleep Hours vs Test Score\n",
    "axes[0, 1].scatter(df['Sleep_Hours'], df['Test_Score'], alpha=0.6, color='green')\n",
    "axes[0, 1].set_xlabel('Sleep Hours')\n",
    "axes[0, 1].set_ylabel('Test Score')\n",
    "axes[0, 1].set_title('Sleep Hours vs Test Score')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "z = np.polyfit(df['Sleep_Hours'], df['Test_Score'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 1].plot(df['Sleep_Hours'], p(df['Sleep_Hours']), \"r--\", alpha=0.8, label='Trend')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Scatter 3: Experience vs Income\n",
    "axes[0, 2].scatter(df['Experience_Years'], df['Income'], alpha=0.6, color='orange')\n",
    "axes[0, 2].set_xlabel('Experience (Years)')\n",
    "axes[0, 2].set_ylabel('Income ($)')\n",
    "axes[0, 2].set_title('Experience vs Income')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "z = np.polyfit(df['Experience_Years'], df['Income'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 2].plot(df['Experience_Years'], p(df['Experience_Years']), \"r--\", alpha=0.8, label='Trend')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Scatter 4: Age vs Income\n",
    "axes[1, 0].scatter(df['Age'], df['Income'], alpha=0.6, color='purple')\n",
    "axes[1, 0].set_xlabel('Age')\n",
    "axes[1, 0].set_ylabel('Income ($)')\n",
    "axes[1, 0].set_title('Age vs Income')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "z = np.polyfit(df['Age'], df['Income'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1, 0].plot(df['Age'], p(df['Age']), \"r--\", alpha=0.8, label='Trend')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Scatter 5: Age vs Test Score\n",
    "axes[1, 1].scatter(df['Age'], df['Test_Score'], alpha=0.6, color='red')\n",
    "axes[1, 1].set_xlabel('Age')\n",
    "axes[1, 1].set_ylabel('Test Score')\n",
    "axes[1, 1].set_title('Age vs Test Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter 6: Hours Studied vs Sleep Hours (colored by Test Score)\n",
    "scatter = axes[1, 2].scatter(df['Hours_Studied'], df['Sleep_Hours'], \n",
    "                             c=df['Test_Score'], cmap='viridis', alpha=0.6)\n",
    "axes[1, 2].set_xlabel('Hours Studied')\n",
    "axes[1, 2].set_ylabel('Sleep Hours')\n",
    "axes[1, 2].set_title('Hours Studied vs Sleep Hours (Color = Test Score)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 2], label='Test Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== LINE CHARTS ====================\n",
    "\n",
    "# Aggregate data by month for time series analysis\n",
    "monthly_data = df.groupby('Month').agg({\n",
    "    'Test_Score': 'mean',\n",
    "    'Hours_Studied': 'mean',\n",
    "    'Sleep_Hours': 'mean',\n",
    "    'Income': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Line Charts - Trends Over Time (Monthly Averages)', fontsize=16)\n",
    "\n",
    "# Line chart 1: Test Score trend\n",
    "axes[0, 0].plot(monthly_data['Month'], monthly_data['Test_Score'], \n",
    "                marker='o', linewidth=2, markersize=8, color='blue')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Average Test Score')\n",
    "axes[0, 0].set_title('Monthly Average Test Score Trend')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(range(1, 13))\n",
    "\n",
    "# Line chart 2: Hours Studied trend\n",
    "axes[0, 1].plot(monthly_data['Month'], monthly_data['Hours_Studied'], \n",
    "                marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Average Hours Studied')\n",
    "axes[0, 1].set_title('Monthly Average Study Hours Trend')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_xticks(range(1, 13))\n",
    "\n",
    "# Line chart 3: Sleep Hours trend\n",
    "axes[1, 0].plot(monthly_data['Month'], monthly_data['Sleep_Hours'], \n",
    "                marker='^', linewidth=2, markersize=8, color='orange')\n",
    "axes[1, 0].set_xlabel('Month')\n",
    "axes[1, 0].set_ylabel('Average Sleep Hours')\n",
    "axes[1, 0].set_title('Monthly Average Sleep Hours Trend')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "\n",
    "# Line chart 4: Multiple metrics comparison\n",
    "axes[1, 1].plot(monthly_data['Month'], monthly_data['Test_Score'], \n",
    "                marker='o', linewidth=2, label='Test Score', color='blue')\n",
    "axes[1, 1].plot(monthly_data['Month'], monthly_data['Hours_Studied'] * 10, \n",
    "                marker='s', linewidth=2, label='Hours Studied (×10)', color='green')\n",
    "axes[1, 1].plot(monthly_data['Month'], monthly_data['Sleep_Hours'] * 10, \n",
    "                marker='^', linewidth=2, label='Sleep Hours (×10)', color='orange')\n",
    "axes[1, 1].set_xlabel('Month')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].set_title('Multi-Metric Comparison (Scaled)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_xticks(range(1, 13))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== CORRELATION HEATMAP ====================\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap - Feature Relationships', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation insights\n",
    "print(\"\\nKey Correlations:\")\n",
    "print(f\"Hours Studied vs Test Score: {correlation_matrix.loc['Hours_Studied', 'Test_Score']:.3f}\")\n",
    "print(f\"Sleep Hours vs Test Score: {correlation_matrix.loc['Sleep_Hours', 'Test_Score']:.3f}\")\n",
    "print(f\"Experience vs Income: {correlation_matrix.loc['Experience_Years', 'Income']:.3f}\")\n",
    "print(f\"Age vs Income: {correlation_matrix.loc['Age', 'Income']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Question 32: Support Vector Machine (SVM) for Classification\n",
    "\n",
    "**Concepts:**\n",
    "- **SVM**: Supervised learning algorithm that finds optimal hyperplane to separate classes\n",
    "- **Hyperplane**: Decision boundary that separates different classes in feature space\n",
    "- **Kernel**: Function to transform data into higher dimensions (linear, RBF, polynomial)\n",
    "- **Support Vectors**: Data points closest to the decision boundary\n",
    "- **RBF Kernel**: Radial Basis Function, useful for non-linear classification\n",
    "- **Decision boundary**: The line/surface that separates different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                             accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# ==================== LOAD AND PREPARE DATA ====================\n",
    "\n",
    "# Load the Iris dataset (classic multi-class classification problem)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target classes: {iris.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"First few rows of dataset:\")\n",
    "print(df.head(10))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ==================== DATA PREPROCESSING ====================\n",
    "\n",
    "# Split dataset into training and testing sets (70-30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                      random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Feature scaling (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Scaling Applied (Standardization)\")\n",
    "print(f\"Training set mean: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"Training set std: {X_train_scaled.std(axis=0)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ==================== TRAIN SVM MODELS ====================\n",
    "\n",
    "# Train SVM with different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svm_models = {}\n",
    "\n",
    "print(\"Training SVM Models with Different Kernels:\\n\")\n",
    "for kernel in kernels:\n",
    "    # Create and train SVM classifier\n",
    "    svm = SVC(kernel=kernel, C=1.0, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    svm_models[kernel] = svm\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{kernel.upper()} Kernel - Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Use RBF kernel for detailed analysis (usually performs best)\n",
    "best_kernel = 'rbf'\n",
    "svm_rbf = svm_models[best_kernel]\n",
    "y_pred = svm_rbf.predict(X_test_scaled)\n",
    "\n",
    "# ==================== MODEL EVALUATION ====================\n",
    "\n",
    "print(f\"Detailed Evaluation for SVM with {best_kernel.upper()} Kernel:\\n\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1-Score: {f1*100:.2f}%\")\n",
    "print(f\"\\nNumber of Support Vectors: {svm_rbf.n_support_}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(svm_rbf, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores (5-fold): {cv_scores}\")\n",
    "print(f\"Mean CV Score: {cv_scores.mean()*100:.2f}% (+/- {cv_scores.std()*100:.2f}%)\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title(f'Confusion Matrix - SVM ({best_kernel.upper()} Kernel)', fontsize=14, pad=20)\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Model Comparison\n",
    "kernel_accuracies = {}\n",
    "for kernel, model in svm_models.items():\n",
    "    y_pred_temp = model.predict(X_test_scaled)\n",
    "    kernel_accuracies[kernel] = accuracy_score(y_test, y_pred_temp)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(kernel_accuracies.keys(), \n",
    "               [acc * 100 for acc in kernel_accuracies.values()],\n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.xlabel('Kernel Type', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.title('SVM Performance Comparison - Different Kernels', fontsize=14, pad=20)\n",
    "plt.ylim([0, 105])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}%', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Decision Boundary Visualization (using first 2 features for 2D plot)\n",
    "# Train a new SVM using only first 2 features for visualization\n",
    "X_2d = X[:, :2]  # Use only first 2 features (sepal length and width)\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the 2D data\n",
    "scaler_2d = StandardScaler()\n",
    "X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "\n",
    "# Train SVM on 2D data\n",
    "svm_2d = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm_2d.fit(X_train_2d_scaled, y_train_2d)\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "h = 0.02  # step size in mesh\n",
    "x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict for each point in mesh\n",
    "Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Decision boundary with training data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "scatter = plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n",
    "                     c=y_train_2d, cmap=plt.cm.RdYlBu, edgecolors='black', s=50)\n",
    "plt.xlabel(f'{iris.feature_names[0]} (scaled)')\n",
    "plt.ylabel(f'{iris.feature_names[1]} (scaled)')\n",
    "plt.title('SVM Decision Boundary - Training Data')\n",
    "plt.colorbar(scatter, label='Class', ticks=[0, 1, 2])\n",
    "\n",
    "# Plot 2: Decision boundary with test data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "scatter = plt.scatter(X_test_2d_scaled[:, 0], X_test_2d_scaled[:, 1], \n",
    "                     c=y_test_2d, cmap=plt.cm.RdYlBu, edgecolors='black', s=50)\n",
    "# Mark misclassified points with red circles\n",
    "y_pred_2d = svm_2d.predict(X_test_2d_scaled)\n",
    "misclassified = X_test_2d_scaled[y_test_2d != y_pred_2d]\n",
    "if len(misclassified) > 0:\n",
    "    plt.scatter(misclassified[:, 0], misclassified[:, 1], \n",
    "               s=200, facecolors='none', edgecolors='red', linewidths=2)\n",
    "plt.xlabel(f'{iris.feature_names[0]} (scaled)')\n",
    "plt.ylabel(f'{iris.feature_names[1]} (scaled)')\n",
    "plt.title('SVM Decision Boundary - Test Data')\n",
    "plt.colorbar(scatter, label='Class', ticks=[0, 1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature Importance Visualization (using all features)\n",
    "# For RBF kernel, we can look at feature scaling impact\n",
    "feature_names = iris.feature_names\n",
    "feature_importance = np.abs(X_train_scaled.mean(axis=0))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(feature_names, feature_importance, color='teal')\n",
    "plt.xlabel('Average Scaled Value (Magnitude)', fontsize=12)\n",
    "plt.title('Feature Scale Distribution After Standardization', fontsize=14, pad=20)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Support Vectors Visualization\n",
    "support_vectors = svm_2d.support_vectors_\n",
    "support_vector_indices = svm_2d.support_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X_train_2d_scaled[:, 0], X_train_2d_scaled[:, 1], \n",
    "           c=y_train_2d, cmap=plt.cm.RdYlBu, edgecolors='black', s=50, alpha=0.7)\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
    "           s=200, facecolors='none', edgecolors='green', linewidths=3, \n",
    "           label=f'Support Vectors (n={len(support_vectors)})')\n",
    "plt.xlabel(f'{iris.feature_names[0]} (scaled)')\n",
    "plt.ylabel(f'{iris.feature_names[1]} (scaled)')\n",
    "plt.title('SVM Support Vectors Highlighted', fontsize=14, pad=20)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization Complete!\")\n",
    "print(f\"Total Support Vectors: {len(support_vectors)}\")\n",
    "print(f\"Support Vectors per class: {svm_2d.n_support_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
