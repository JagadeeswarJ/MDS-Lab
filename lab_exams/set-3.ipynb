{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Exam - Set 3\n",
    "\n",
    "This notebook contains implementations for all questions in Set 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9: NumPy Arrays - Indexing, Slicing, and Universal Functions\n",
    "\n",
    "**Concepts:**\n",
    "- **Creating from lists**: Convert Python lists to NumPy arrays\n",
    "- **Indexing**: Accessing single elements using position\n",
    "- **Slicing**: Extracting subarrays using start:stop:step\n",
    "- **Universal functions (ufuncs)**: Fast element-wise operations (sqrt, exp, sin, etc.)\n",
    "- **Broadcasting**: Operating on arrays of different shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create NumPy arrays from lists\n",
    "list1 = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "list2 = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
    "\n",
    "arr1 = np.array(list1)\n",
    "arr2 = np.array(list2)\n",
    "\n",
    "print(\"Array 1 (1D):\", arr1)\n",
    "print(\"\\nArray 2 (2D):\\n\", arr2)\n",
    "\n",
    "# Indexing operations\n",
    "print(\"\\n\\n=== INDEXING ===\")\n",
    "print(f\"First element of arr1: {arr1[0]}\")\n",
    "print(f\"Last element of arr1: {arr1[-1]}\")\n",
    "print(f\"Element at position 5: {arr1[5]}\")\n",
    "print(f\"\\nElement at row 1, col 2 in arr2: {arr2[1, 2]}\")\n",
    "print(f\"First row of arr2: {arr2[0]}\")\n",
    "print(f\"Last column of arr2: {arr2[:, -1]}\")\n",
    "\n",
    "# Slicing operations\n",
    "print(\"\\n\\n=== SLICING ===\")\n",
    "print(f\"First 5 elements: {arr1[:5]}\")\n",
    "print(f\"Elements from index 3 to 7: {arr1[3:8]}\")\n",
    "print(f\"Every 2nd element: {arr1[::2]}\")\n",
    "print(f\"Last 3 elements: {arr1[-3:]}\")\n",
    "print(f\"Reverse array: {arr1[::-1]}\")\n",
    "\n",
    "print(f\"\\nFirst 2 rows of arr2:\\n{arr2[:2]}\")\n",
    "print(f\"\\nFirst 2 columns:\\n{arr2[:, :2]}\")\n",
    "print(f\"\\nCenter 2x2 subarray:\\n{arr2[0:2, 1:3]}\")\n",
    "\n",
    "# Universal functions (ufuncs)\n",
    "print(\"\\n\\n=== UNIVERSAL FUNCTIONS ===\")\n",
    "arr3 = np.array([1, 4, 9, 16, 25])\n",
    "print(f\"Original array: {arr3}\")\n",
    "\n",
    "# Mathematical ufuncs\n",
    "print(f\"Square root: {np.sqrt(arr3)}\")\n",
    "print(f\"Square: {np.square(arr3)}\")\n",
    "print(f\"Exponential: {np.exp([1, 2, 3])}\")\n",
    "print(f\"Log (natural): {np.log(arr3)}\")\n",
    "print(f\"Log10: {np.log10(arr3)}\")\n",
    "\n",
    "# Trigonometric ufuncs\n",
    "angles = np.array([0, 30, 45, 60, 90])\n",
    "radians = np.deg2rad(angles)\n",
    "print(f\"\\nAngles: {angles}°\")\n",
    "print(f\"Sin values: {np.sin(radians)}\")\n",
    "print(f\"Cos values: {np.cos(radians)}\")\n",
    "\n",
    "# Statistical ufuncs\n",
    "print(f\"\\nArray 1: {arr1}\")\n",
    "print(f\"Sum: {np.sum(arr1)}\")\n",
    "print(f\"Mean: {np.mean(arr1)}\")\n",
    "print(f\"Median: {np.median(arr1)}\")\n",
    "print(f\"Std deviation: {np.std(arr1)}\")\n",
    "print(f\"Min: {np.min(arr1)}\")\n",
    "print(f\"Max: {np.max(arr1)}\")\n",
    "\n",
    "# Comparison and logical ufuncs\n",
    "print(f\"\\nElements > 50: {arr1[arr1 > 50]}\")\n",
    "print(f\"Boolean mask (>50): {arr1 > 50}\")\n",
    "print(f\"Count of elements > 50: {np.sum(arr1 > 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Label Encoding and Missing Value Imputation\n",
    "\n",
    "**Concepts:**\n",
    "- **Categorical data**: Non-numeric data (colors, categories, names)\n",
    "- **Label encoding**: Converting categories to numbers (0, 1, 2, ...)\n",
    "- **Imputation**: Filling missing values with statistical measures\n",
    "- **SimpleImputer**: sklearn tool for handling missing values\n",
    "- **Strategies**: mean, median, most_frequent, constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create dataset with categorical data and missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Gender': ['Female', 'Male', 'Male', np.nan, 'Female', 'Male', 'Female', 'Male'],\n",
    "    'City': ['Mumbai', 'Delhi', np.nan, 'Mumbai', 'Bangalore', 'Delhi', 'Mumbai', np.nan],\n",
    "    'Education': ['Graduate', 'Post-Graduate', 'Graduate', 'High School', np.nan, 'Graduate', 'Post-Graduate', 'Graduate'],\n",
    "    'Age': [25, np.nan, 30, 28, 35, np.nan, 27, 29],\n",
    "    'Salary': [50000, 75000, np.nan, 60000, np.nan, 55000, 70000, 65000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n\\nMissing values count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Label Encoding for categorical columns\n",
    "print(\"\\n\\n=== LABEL ENCODING ===\")\n",
    "\n",
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode Gender\n",
    "le_gender = LabelEncoder()\n",
    "# Remove NaN for encoding, then add back\n",
    "mask_gender = df_encoded['Gender'].notna()\n",
    "df_encoded.loc[mask_gender, 'Gender_Encoded'] = le_gender.fit_transform(df_encoded.loc[mask_gender, 'Gender'])\n",
    "print(f\"\\nGender Encoding: {dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_)))}\")\n",
    "\n",
    "# Encode City\n",
    "le_city = LabelEncoder()\n",
    "mask_city = df_encoded['City'].notna()\n",
    "df_encoded.loc[mask_city, 'City_Encoded'] = le_city.fit_transform(df_encoded.loc[mask_city, 'City'])\n",
    "print(f\"City Encoding: {dict(zip(le_city.classes_, le_city.transform(le_city.classes_)))}\")\n",
    "\n",
    "# Encode Education\n",
    "le_education = LabelEncoder()\n",
    "mask_education = df_encoded['Education'].notna()\n",
    "df_encoded.loc[mask_education, 'Education_Encoded'] = le_education.fit_transform(df_encoded.loc[mask_education, 'Education'])\n",
    "print(f\"Education Encoding: {dict(zip(le_education.classes_, le_education.transform(le_education.classes_)))}\")\n",
    "\n",
    "print(\"\\n\\nDataFrame with Encoded Columns:\")\n",
    "print(df_encoded)\n",
    "\n",
    "# Handle missing values using imputation\n",
    "print(\"\\n\\n=== MISSING VALUE IMPUTATION ===\")\n",
    "\n",
    "# For numerical columns - use mean imputation\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df_encoded[['Age', 'Salary']] = imputer_mean.fit_transform(df_encoded[['Age', 'Salary']])\n",
    "\n",
    "print(f\"\\nImputed Age values (using mean={imputer_mean.statistics_[0]:.2f}):\")\n",
    "print(df_encoded['Age'])\n",
    "\n",
    "print(f\"\\nImputed Salary values (using mean={imputer_mean.statistics_[1]:.2f}):\")\n",
    "print(df_encoded['Salary'])\n",
    "\n",
    "# For categorical encoded columns - use most frequent\n",
    "imputer_freq = SimpleImputer(strategy='most_frequent')\n",
    "df_encoded[['Gender_Encoded', 'City_Encoded', 'Education_Encoded']] = imputer_freq.fit_transform(\n",
    "    df_encoded[['Gender_Encoded', 'City_Encoded', 'Education_Encoded']]\n",
    ")\n",
    "\n",
    "print(\"\\n\\nFinal Dataset (After Encoding and Imputation):\")\n",
    "print(df_encoded[['Name', 'Age', 'Salary', 'Gender_Encoded', 'City_Encoded', 'Education_Encoded']])\n",
    "\n",
    "print(\"\\n\\nMissing values after imputation:\")\n",
    "print(df_encoded.isnull().sum())\n",
    "\n",
    "print(\"\\n\\nSummary Statistics:\")\n",
    "print(df_encoded[['Age', 'Salary', 'Gender_Encoded', 'City_Encoded', 'Education_Encoded']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11: Scatter Plots with Regression Lines and Bar Plots with Error Bars\n",
    "\n",
    "**Concepts:**\n",
    "- **Regression line**: Best-fit line showing trend between variables\n",
    "- **Linear regression**: Finding line that minimizes error\n",
    "- **Error bars**: Show variability/uncertainty in data\n",
    "- **Standard error**: Measure of variability in sample mean\n",
    "- **Confidence intervals**: Range where true value likely lies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Create dataset\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "\n",
    "# Feature relationships with noise\n",
    "advertising_spend = np.random.uniform(1000, 10000, n)\n",
    "sales = 5000 + 0.8 * advertising_spend + np.random.normal(0, 1000, n)\n",
    "\n",
    "study_hours = np.random.uniform(1, 10, n)\n",
    "exam_score = 30 + 6 * study_hours + np.random.normal(0, 5, n)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Advertising': advertising_spend,\n",
    "    'Sales': sales,\n",
    "    'Study_Hours': study_hours,\n",
    "    'Exam_Score': exam_score\n",
    "})\n",
    "\n",
    "# Scatter plots with regression lines\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Scatter Plots with Regression Lines', fontsize=16)\n",
    "\n",
    "# Plot 1: Advertising vs Sales\n",
    "axes[0].scatter(df['Advertising'], df['Sales'], alpha=0.6, s=50, color='blue', label='Data points')\n",
    "# Calculate regression line\n",
    "slope1, intercept1, r_value1, p_value1, std_err1 = stats.linregress(df['Advertising'], df['Sales'])\n",
    "line1 = slope1 * df['Advertising'] + intercept1\n",
    "axes[0].plot(df['Advertising'], line1, 'r-', linewidth=2, label=f'Regression line\\ny={slope1:.2f}x+{intercept1:.0f}')\n",
    "axes[0].set_xlabel('Advertising Spend ($)', fontsize=11)\n",
    "axes[0].set_ylabel('Sales ($)', fontsize=11)\n",
    "axes[0].set_title(f'Advertising vs Sales\\nR² = {r_value1**2:.3f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Study Hours vs Exam Score\n",
    "axes[1].scatter(df['Study_Hours'], df['Exam_Score'], alpha=0.6, s=50, color='green', label='Data points')\n",
    "slope2, intercept2, r_value2, p_value2, std_err2 = stats.linregress(df['Study_Hours'], df['Exam_Score'])\n",
    "line2 = slope2 * df['Study_Hours'] + intercept2\n",
    "axes[1].plot(df['Study_Hours'], line2, 'r-', linewidth=2, label=f'Regression line\\ny={slope2:.2f}x+{intercept2:.0f}')\n",
    "axes[1].set_xlabel('Study Hours', fontsize=11)\n",
    "axes[1].set_ylabel('Exam Score', fontsize=11)\n",
    "axes[1].set_title(f'Study Hours vs Exam Score\\nR² = {r_value2**2:.3f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plots with error bars\n",
    "# Create categorical data\n",
    "categories = ['Product A', 'Product B', 'Product C', 'Product D']\n",
    "samples_per_category = 20\n",
    "\n",
    "data_cat = {\n",
    "    'Product A': np.random.normal(75, 10, samples_per_category),\n",
    "    'Product B': np.random.normal(85, 8, samples_per_category),\n",
    "    'Product C': np.random.normal(70, 12, samples_per_category),\n",
    "    'Product D': np.random.normal(90, 7, samples_per_category)\n",
    "}\n",
    "\n",
    "# Calculate means and standard errors\n",
    "means = [np.mean(data_cat[cat]) for cat in categories]\n",
    "std_errors = [stats.sem(data_cat[cat]) for cat in categories]  # Standard error of mean\n",
    "std_devs = [np.std(data_cat[cat]) for cat in categories]\n",
    "\n",
    "# Create bar plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Bar Plots with Error Bars', fontsize=16)\n",
    "\n",
    "# Plot 1: With standard error bars\n",
    "axes[0].bar(categories, means, color=['skyblue', 'lightcoral', 'lightgreen', 'plum'], \n",
    "            alpha=0.7, edgecolor='black')\n",
    "axes[0].errorbar(categories, means, yerr=std_errors, fmt='none', \n",
    "                 ecolor='black', capsize=5, capthick=2, label='Standard Error')\n",
    "axes[0].set_xlabel('Product Category', fontsize=11)\n",
    "axes[0].set_ylabel('Average Performance Score', fontsize=11)\n",
    "axes[0].set_title('Product Performance with Standard Error Bars')\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (cat, mean, se) in enumerate(zip(categories, means, std_errors)):\n",
    "    axes[0].text(i, mean + se + 2, f'{mean:.1f}±{se:.1f}', \n",
    "                ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: With standard deviation bars\n",
    "axes[1].bar(categories, means, color=['skyblue', 'lightcoral', 'lightgreen', 'plum'], \n",
    "            alpha=0.7, edgecolor='black')\n",
    "axes[1].errorbar(categories, means, yerr=std_devs, fmt='none', \n",
    "                 ecolor='red', capsize=5, capthick=2, label='Standard Deviation')\n",
    "axes[1].set_xlabel('Product Category', fontsize=11)\n",
    "axes[1].set_ylabel('Average Performance Score', fontsize=11)\n",
    "axes[1].set_title('Product Performance with Standard Deviation Bars')\n",
    "axes[1].set_ylim(0, 110)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (cat, mean, sd) in enumerate(zip(categories, means, std_devs)):\n",
    "    axes[1].text(i, mean + sd + 3, f'{mean:.1f}±{sd:.1f}', \n",
    "                ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"\\nProduct Performance Statistics:\")\n",
    "for cat in categories:\n",
    "    mean_val = np.mean(data_cat[cat])\n",
    "    se_val = stats.sem(data_cat[cat])\n",
    "    sd_val = np.std(data_cat[cat])\n",
    "    print(f\"{cat}: Mean={mean_val:.2f}, SE={se_val:.2f}, SD={sd_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12: Decision Tree Classifier using Gini Index\n",
    "\n",
    "**Concepts:**\n",
    "- **Decision Tree**: Tree-like model making decisions based on feature values\n",
    "- **Gini Index**: Measure of impurity (0=pure, 0.5=maximum impurity)\n",
    "- **Splitting**: Dividing data based on feature thresholds\n",
    "- **Leaf nodes**: Final classification decisions\n",
    "- **Tree depth**: Number of levels in the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Create Decision Tree using Gini Index\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    criterion='gini',  # Use Gini Index for splitting\n",
    "    max_depth=4,       # Limit tree depth\n",
    "    random_state=42,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Decision Tree Classifier (Gini Index)', fontsize=14)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = dt_classifier.feature_importances_\n",
    "print(\"\\nFeature Importance:\")\n",
    "for feature, importance in zip(iris.feature_names, feature_importance):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(iris.feature_names, feature_importance, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance Score', fontsize=12)\n",
    "plt.title('Feature Importance in Decision Tree', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the Decision Tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_classifier, \n",
    "          feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree Visualization (Gini Index)', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tree statistics\n",
    "print(f\"\\n\\nTree Statistics:\")\n",
    "print(f\"Tree depth: {dt_classifier.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_classifier.get_n_leaves()}\")\n",
    "print(f\"Total number of nodes: {dt_classifier.tree_.node_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
