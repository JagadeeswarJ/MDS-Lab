{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Lab Exam - Set 5\n",
    "\n",
    "This notebook contains implementations for all questions in Set 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## Question 17: NumPy Arrays - Arithmetic and Statistical Operations with Universal Functions\n",
    "\n",
    "**Concepts:**\n",
    "- **NumPy Arrays**: Efficient multi-dimensional containers for numerical data\n",
    "- **Universal Functions (ufuncs)**: Fast element-wise operations on arrays\n",
    "- **Arithmetic operations**: Addition, subtraction, multiplication, division, power, modulo\n",
    "- **Statistical operations**: Mean, median, standard deviation, variance, min, max, percentiles\n",
    "- **Broadcasting**: NumPy's ability to perform operations on arrays of different shapes\n",
    "- **Aggregation functions**: Sum, product, cumulative sum, cumulative product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create NumPy arrays\n",
    "arr1 = np.array([10, 20, 30, 40, 50])\n",
    "arr2 = np.array([1, 2, 3, 4, 5])\n",
    "arr3 = np.random.randint(1, 100, size=(5, 5))  # 5x5 random array\n",
    "\n",
    "print(\"Array 1:\", arr1)\n",
    "print(\"Array 2:\", arr2)\n",
    "print(\"\\nArray 3 (5x5 random):\")\n",
    "print(arr3)\n",
    "\n",
    "# ========== ARITHMETIC OPERATIONS USING UNIVERSAL FUNCTIONS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARITHMETIC OPERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Addition\n",
    "add_result = np.add(arr1, arr2)\n",
    "print(\"\\nAddition (arr1 + arr2):\", add_result)\n",
    "\n",
    "# Subtraction\n",
    "sub_result = np.subtract(arr1, arr2)\n",
    "print(\"Subtraction (arr1 - arr2):\", sub_result)\n",
    "\n",
    "# Multiplication\n",
    "mul_result = np.multiply(arr1, arr2)\n",
    "print(\"Multiplication (arr1 * arr2):\", mul_result)\n",
    "\n",
    "# Division\n",
    "div_result = np.divide(arr1, arr2)\n",
    "print(\"Division (arr1 / arr2):\", div_result)\n",
    "\n",
    "# Power\n",
    "power_result = np.power(arr2, 2)\n",
    "print(\"Power (arr2 ** 2):\", power_result)\n",
    "\n",
    "# Modulo\n",
    "mod_result = np.mod(arr1, arr2)\n",
    "print(\"Modulo (arr1 % arr2):\", mod_result)\n",
    "\n",
    "# Square root\n",
    "sqrt_result = np.sqrt(arr1)\n",
    "print(\"Square root of arr1:\", sqrt_result)\n",
    "\n",
    "# Exponential\n",
    "exp_result = np.exp(arr2)\n",
    "print(\"Exponential of arr2:\", exp_result)\n",
    "\n",
    "# Logarithm\n",
    "log_result = np.log(arr1)\n",
    "print(\"Natural log of arr1:\", log_result)\n",
    "\n",
    "# Absolute value\n",
    "arr_negative = np.array([-5, -10, 15, -20, 25])\n",
    "abs_result = np.abs(arr_negative)\n",
    "print(\"\\nAbsolute value of [-5, -10, 15, -20, 25]:\", abs_result)\n",
    "\n",
    "# ========== STATISTICAL OPERATIONS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL OPERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Using arr3 for statistical operations\n",
    "print(\"\\nStatistics on Array 3:\")\n",
    "\n",
    "# Mean\n",
    "mean_all = np.mean(arr3)\n",
    "mean_axis0 = np.mean(arr3, axis=0)  # Column-wise mean\n",
    "mean_axis1 = np.mean(arr3, axis=1)  # Row-wise mean\n",
    "print(f\"Mean (overall): {mean_all:.2f}\")\n",
    "print(f\"Mean (column-wise): {mean_axis0}\")\n",
    "print(f\"Mean (row-wise): {mean_axis1}\")\n",
    "\n",
    "# Median\n",
    "median_all = np.median(arr3)\n",
    "print(f\"\\nMedian (overall): {median_all:.2f}\")\n",
    "\n",
    "# Standard Deviation\n",
    "std_all = np.std(arr3)\n",
    "print(f\"Standard Deviation: {std_all:.2f}\")\n",
    "\n",
    "# Variance\n",
    "var_all = np.var(arr3)\n",
    "print(f\"Variance: {var_all:.2f}\")\n",
    "\n",
    "# Minimum and Maximum\n",
    "min_val = np.min(arr3)\n",
    "max_val = np.max(arr3)\n",
    "print(f\"\\nMinimum: {min_val}\")\n",
    "print(f\"Maximum: {max_val}\")\n",
    "\n",
    "# Minimum and Maximum indices\n",
    "min_idx = np.argmin(arr3)\n",
    "max_idx = np.argmax(arr3)\n",
    "print(f\"Minimum index (flattened): {min_idx}\")\n",
    "print(f\"Maximum index (flattened): {max_idx}\")\n",
    "\n",
    "# Percentiles\n",
    "percentile_25 = np.percentile(arr3, 25)\n",
    "percentile_50 = np.percentile(arr3, 50)  # Same as median\n",
    "percentile_75 = np.percentile(arr3, 75)\n",
    "print(f\"\\n25th Percentile: {percentile_25:.2f}\")\n",
    "print(f\"50th Percentile (Median): {percentile_50:.2f}\")\n",
    "print(f\"75th Percentile: {percentile_75:.2f}\")\n",
    "\n",
    "# Sum and Product\n",
    "sum_all = np.sum(arr3)\n",
    "prod_arr2 = np.prod(arr2)\n",
    "print(f\"\\nSum of all elements in arr3: {sum_all}\")\n",
    "print(f\"Product of all elements in arr2: {prod_arr2}\")\n",
    "\n",
    "# Cumulative sum and product\n",
    "cumsum_arr2 = np.cumsum(arr2)\n",
    "cumprod_arr2 = np.cumprod(arr2)\n",
    "print(f\"\\nCumulative sum of arr2: {cumsum_arr2}\")\n",
    "print(f\"Cumulative product of arr2: {cumprod_arr2}\")\n",
    "\n",
    "# ========== ADDITIONAL USEFUL OPERATIONS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL OPERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sorting\n",
    "sorted_arr = np.sort(arr1)\n",
    "print(f\"\\nSorted arr1: {sorted_arr}\")\n",
    "\n",
    "# Unique values\n",
    "arr_duplicates = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5])\n",
    "unique_vals = np.unique(arr_duplicates)\n",
    "print(f\"Unique values in [1,2,2,3,3,3,4,4,5]: {unique_vals}\")\n",
    "\n",
    "# Correlation coefficient\n",
    "corr_coef = np.corrcoef(arr1, arr2)\n",
    "print(f\"\\nCorrelation coefficient matrix:\")\n",
    "print(corr_coef)\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SUMMARY FOR ARRAY 3\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {arr3.shape}\")\n",
    "print(f\"Size: {arr3.size}\")\n",
    "print(f\"Data type: {arr3.dtype}\")\n",
    "print(f\"Mean: {np.mean(arr3):.2f}\")\n",
    "print(f\"Median: {np.median(arr3):.2f}\")\n",
    "print(f\"Std Dev: {np.std(arr3):.2f}\")\n",
    "print(f\"Variance: {np.var(arr3):.2f}\")\n",
    "print(f\"Min: {np.min(arr3)}\")\n",
    "print(f\"Max: {np.max(arr3)}\")\n",
    "print(f\"Range: {np.ptp(arr3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "## Question 18: Pandas DataFrame - Import CSV, Drop Duplicates, and Group-wise Statistics\n",
    "\n",
    "**Concepts:**\n",
    "- **CSV Import**: Reading comma-separated values files using `pd.read_csv()`\n",
    "- **Duplicate Records**: Rows with identical values in all or specific columns\n",
    "- **drop_duplicates()**: Method to remove duplicate rows from DataFrame\n",
    "- **GroupBy**: Splitting data into groups based on criteria\n",
    "- **Aggregation**: Computing summary statistics for each group\n",
    "- **Group-wise statistics**: Mean, sum, count, min, max for different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample CSV file with duplicates and multiple categories\n",
    "data = {\n",
    "    'Employee_ID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 105, 111, 112],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry', \n",
    "             'Ivy', 'Jack', 'Bob', 'Eve', 'Kelly', 'Liam'],\n",
    "    'Department': ['Sales', 'IT', 'Sales', 'HR', 'IT', 'Sales', 'HR', 'IT', \n",
    "                   'Sales', 'HR', 'IT', 'IT', 'Sales', 'HR'],\n",
    "    'Age': [28, 35, 42, 29, 31, 38, 27, 33, 45, 30, 35, 31, 26, 34],\n",
    "    'Salary': [50000, 75000, 60000, 55000, 72000, 58000, 53000, 78000, \n",
    "               62000, 54000, 75000, 72000, 51000, 56000],\n",
    "    'Experience': [3, 8, 15, 5, 7, 10, 4, 9, 18, 6, 8, 7, 2, 7]\n",
    "}\n",
    "\n",
    "df_sample = pd.DataFrame(data)\n",
    "df_sample.to_csv('employee_data.csv', index=False)\n",
    "print(\"Sample CSV file 'employee_data.csv' created successfully!\\n\")\n",
    "\n",
    "# ========== IMPORT CSV FILE ==========\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: IMPORT CSV FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv('employee_data.csv')\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# ========== IDENTIFY AND DROP DUPLICATE RECORDS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: IDENTIFY AND DROP DUPLICATE RECORDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nTotal duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Show duplicate rows\n",
    "duplicates = df[df.duplicated(keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"\\nDuplicate records found:\")\n",
    "    print(duplicates.sort_values('Employee_ID'))\n",
    "\n",
    "# Check duplicates based on specific columns (Employee_ID)\n",
    "print(f\"\\nDuplicate Employee_IDs: {df.duplicated(subset=['Employee_ID']).sum()}\")\n",
    "\n",
    "# Show which Employee_IDs are duplicated\n",
    "duplicate_ids = df[df.duplicated(subset=['Employee_ID'], keep=False)]\n",
    "if not duplicate_ids.empty:\n",
    "    print(\"\\nRecords with duplicate Employee_IDs:\")\n",
    "    print(duplicate_ids.sort_values('Employee_ID'))\n",
    "\n",
    "# Drop duplicates based on Employee_ID (keep first occurrence)\n",
    "df_cleaned = df.drop_duplicates(subset=['Employee_ID'], keep='first')\n",
    "\n",
    "print(f\"\\n\\nAfter removing duplicates:\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "print(f\"Records removed: {len(df) - len(df_cleaned)}\")\n",
    "\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# ========== COMPUTE GROUP-WISE STATISTICS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: COMPUTE GROUP-WISE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Group by Department and compute various statistics\n",
    "print(\"\\n1. GROUP BY DEPARTMENT - MEAN VALUES:\")\n",
    "print(\"=\"*50)\n",
    "dept_mean = df_cleaned.groupby('Department')[['Age', 'Salary', 'Experience']].mean()\n",
    "print(dept_mean)\n",
    "\n",
    "print(\"\\n2. GROUP BY DEPARTMENT - SUM VALUES:\")\n",
    "print(\"=\"*50)\n",
    "dept_sum = df_cleaned.groupby('Department')[['Salary']].sum()\n",
    "print(dept_sum)\n",
    "\n",
    "print(\"\\n3. GROUP BY DEPARTMENT - COUNT:\")\n",
    "print(\"=\"*50)\n",
    "dept_count = df_cleaned.groupby('Department').size()\n",
    "print(dept_count)\n",
    "\n",
    "print(\"\\n4. GROUP BY DEPARTMENT - MIN AND MAX SALARY:\")\n",
    "print(\"=\"*50)\n",
    "dept_minmax = df_cleaned.groupby('Department')['Salary'].agg(['min', 'max', 'mean'])\n",
    "print(dept_minmax)\n",
    "\n",
    "print(\"\\n5. GROUP BY DEPARTMENT - MULTIPLE AGGREGATIONS:\")\n",
    "print(\"=\"*50)\n",
    "dept_agg = df_cleaned.groupby('Department').agg({\n",
    "    'Age': ['mean', 'min', 'max'],\n",
    "    'Salary': ['mean', 'sum', 'std'],\n",
    "    'Experience': ['mean', 'median']\n",
    "})\n",
    "print(dept_agg)\n",
    "\n",
    "print(\"\\n6. GROUP BY DEPARTMENT - DETAILED STATISTICS:\")\n",
    "print(\"=\"*50)\n",
    "dept_describe = df_cleaned.groupby('Department')['Salary'].describe()\n",
    "print(dept_describe)\n",
    "\n",
    "# Additional grouping examples\n",
    "print(\"\\n7. GROUP BY AGE RANGES:\")\n",
    "print(\"=\"*50)\n",
    "# Create age groups\n",
    "df_cleaned['Age_Group'] = pd.cut(df_cleaned['Age'], \n",
    "                                   bins=[0, 30, 40, 100], \n",
    "                                   labels=['Young', 'Middle', 'Senior'])\n",
    "age_group_stats = df_cleaned.groupby('Age_Group')[['Salary', 'Experience']].mean()\n",
    "print(age_group_stats)\n",
    "\n",
    "# Save cleaned data\n",
    "df_cleaned.to_csv('employee_data_cleaned.csv', index=False)\n",
    "print(\"\\n\\nCleaned data saved to 'employee_data_cleaned.csv'\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original records: {len(df)}\")\n",
    "print(f\"Duplicate records removed: {len(df) - len(df_cleaned)}\")\n",
    "print(f\"Final records: {len(df_cleaned)}\")\n",
    "print(f\"Departments: {df_cleaned['Department'].nunique()}\")\n",
    "print(f\"Department names: {df_cleaned['Department'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1v2w3x4",
   "metadata": {},
   "source": [
    "## Question 19: Overlapping Histograms - Compare Distributions Among Features\n",
    "\n",
    "**Concepts:**\n",
    "- **Histogram**: Graphical representation of data distribution using bins\n",
    "- **Overlapping plots**: Multiple distributions on same axes for comparison\n",
    "- **Distribution comparison**: Analyzing shape, center, and spread of different features\n",
    "- **Alpha transparency**: Making overlapping plots visible using transparency\n",
    "- **Matplotlib**: Python library for creating visualizations\n",
    "- **Seaborn**: Statistical data visualization library built on matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create sample dataset with multiple features\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "data = {\n",
    "    'Math_Score': np.random.normal(75, 10, n_samples),\n",
    "    'Physics_Score': np.random.normal(70, 12, n_samples),\n",
    "    'Chemistry_Score': np.random.normal(72, 11, n_samples),\n",
    "    'English_Score': np.random.normal(68, 15, n_samples),\n",
    "    'Category': np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure scores are within 0-100 range\n",
    "for col in ['Math_Score', 'Physics_Score', 'Chemistry_Score', 'English_Score']:\n",
    "    df[col] = df[col].clip(0, 100)\n",
    "\n",
    "print(\"Dataset created with multiple features:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# ========== PLOT 1: OVERLAPPING HISTOGRAMS FOR ALL SUBJECTS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERLAPPING HISTOGRAMS - COMPARING ALL SUBJECTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot overlapping histograms with transparency\n",
    "plt.hist(df['Math_Score'], bins=20, alpha=0.5, label='Math', color='blue', edgecolor='black')\n",
    "plt.hist(df['Physics_Score'], bins=20, alpha=0.5, label='Physics', color='red', edgecolor='black')\n",
    "plt.hist(df['Chemistry_Score'], bins=20, alpha=0.5, label='Chemistry', color='green', edgecolor='black')\n",
    "plt.hist(df['English_Score'], bins=20, alpha=0.5, label='English', color='orange', edgecolor='black')\n",
    "\n",
    "plt.xlabel('Score', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Overlapping Histograms - Subject Score Distributions', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== PLOT 2: OVERLAPPING DENSITY PLOTS ==========\n",
    "print(\"\\nDensity Plots for better comparison...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot density curves\n",
    "df['Math_Score'].plot(kind='density', label='Math', color='blue', linewidth=2)\n",
    "df['Physics_Score'].plot(kind='density', label='Physics', color='red', linewidth=2)\n",
    "df['Chemistry_Score'].plot(kind='density', label='Chemistry', color='green', linewidth=2)\n",
    "df['English_Score'].plot(kind='density', label='English', color='orange', linewidth=2)\n",
    "\n",
    "plt.xlabel('Score', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Overlapping Density Plots - Subject Score Distributions', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== PLOT 3: COMBINED HISTOGRAM AND DENSITY ==========\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "subjects = ['Math_Score', 'Physics_Score', 'Chemistry_Score', 'English_Score']\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for idx, (subject, color) in enumerate(zip(subjects, colors)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    # Histogram\n",
    "    axes[row, col].hist(df[subject], bins=20, alpha=0.6, color=color, \n",
    "                        edgecolor='black', density=True, label='Histogram')\n",
    "    \n",
    "    # Density plot\n",
    "    df[subject].plot(kind='density', ax=axes[row, col], color='darkblue', \n",
    "                     linewidth=2, label='Density')\n",
    "    \n",
    "    axes[row, col].set_xlabel('Score', fontsize=10)\n",
    "    axes[row, col].set_ylabel('Density/Frequency', fontsize=10)\n",
    "    axes[row, col].set_title(f'{subject.replace(\"_\", \" \")} Distribution', \n",
    "                             fontsize=11, fontweight='bold')\n",
    "    axes[row, col].legend(fontsize=9)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Individual Subject Distributions - Histogram + Density', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== PLOT 4: OVERLAPPING HISTOGRAMS BY CATEGORY ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERLAPPING HISTOGRAMS - COMPARING CATEGORIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, subject in enumerate(subjects):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    # Plot histogram for each category\n",
    "    for category in ['A', 'B', 'C']:\n",
    "        data_cat = df[df['Category'] == category][subject]\n",
    "        axes[row, col].hist(data_cat, bins=15, alpha=0.5, \n",
    "                           label=f'Category {category}', edgecolor='black')\n",
    "    \n",
    "    axes[row, col].set_xlabel('Score', fontsize=10)\n",
    "    axes[row, col].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[row, col].set_title(f'{subject.replace(\"_\", \" \")} by Category', \n",
    "                             fontsize=11, fontweight='bold')\n",
    "    axes[row, col].legend(fontsize=9)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Overlapping Histograms - Comparison by Category', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== PLOT 5: SEABORN DISTPLOT (MODERN APPROACH) ==========\n",
    "print(\"\\nAdvanced visualization using Seaborn...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# KDE plot\n",
    "for subject, color in zip(subjects, colors):\n",
    "    sns.kdeplot(data=df, x=subject, label=subject.replace('_Score', ''), \n",
    "                color=color, linewidth=2, ax=axes[0])\n",
    "\n",
    "axes[0].set_xlabel('Score', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('KDE Plot - All Subjects', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot for comparison\n",
    "df_melted = df.melt(value_vars=subjects, var_name='Subject', value_name='Score')\n",
    "sns.boxplot(data=df_melted, x='Subject', y='Score', ax=axes[1], palette='Set2')\n",
    "axes[1].set_xlabel('Subject', fontsize=11)\n",
    "axes[1].set_ylabel('Score', fontsize=11)\n",
    "axes[1].set_title('Box Plot - Score Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticklabels([s.replace('_Score', '') for s in subjects], rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== STATISTICAL COMPARISON ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL COMPARISON OF DISTRIBUTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_stats = pd.DataFrame({\n",
    "    'Subject': subjects,\n",
    "    'Mean': [df[s].mean() for s in subjects],\n",
    "    'Median': [df[s].median() for s in subjects],\n",
    "    'Std Dev': [df[s].std() for s in subjects],\n",
    "    'Min': [df[s].min() for s in subjects],\n",
    "    'Max': [df[s].max() for s in subjects]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison_stats.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Distribution Shape: All subjects show approximately normal distribution\")\n",
    "print(\"2. Central Tendency: Math has highest mean, English has lowest\")\n",
    "print(\"3. Spread: English has highest variability (std dev), Math has lowest\")\n",
    "print(\"4. Overlap: Significant overlap between distributions indicates similar performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Question 20: Logistic Regression for Binary Classification - Confusion Matrix and ROC Curve\n",
    "\n",
    "**Concepts:**\n",
    "- **Logistic Regression**: Supervised learning algorithm for binary classification\n",
    "- **Binary Classification**: Predicting one of two possible outcomes (0 or 1, Yes or No)\n",
    "- **Confusion Matrix**: Table showing True Positives, True Negatives, False Positives, False Negatives\n",
    "- **ROC Curve**: Receiver Operating Characteristic curve showing TPR vs FPR\n",
    "- **AUC**: Area Under the ROC Curve - measures model performance (0.5 to 1.0)\n",
    "- **Accuracy**: (TP + TN) / Total predictions\n",
    "- **Precision**: TP / (TP + FP)\n",
    "- **Recall**: TP / (TP + FN)\n",
    "- **F1-Score**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========== CREATE BINARY CLASSIFICATION DATASET ==========\n",
    "print(\"=\"*60)\n",
    "print(\"CREATING BINARY CLASSIFICATION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                          n_features=4, \n",
    "                          n_informative=3, \n",
    "                          n_redundant=1, \n",
    "                          n_classes=2, \n",
    "                          random_state=42,\n",
    "                          class_sep=1.0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "feature_names = ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4']\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Target'] = y\n",
    "\n",
    "print(\"\\nDataset created successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Target'].value_counts())\n",
    "print(f\"\\nClass 0: {(y == 0).sum()} samples\")\n",
    "print(f\"Class 1: {(y == 1).sum()} samples\")\n",
    "\n",
    "# ========== DATA PREPROCESSING ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split features and target\n",
    "X = df[feature_names]\n",
    "y = df['Target']\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                      random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Feature scaling (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature scaling completed!\")\n",
    "\n",
    "# ========== TRAIN LOGISTIC REGRESSION MODEL ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create and train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nModel trained successfully!\")\n",
    "print(f\"\\nModel coefficients: {log_reg.coef_[0]}\")\n",
    "print(f\"Model intercept: {log_reg.intercept_[0]}\")\n",
    "\n",
    "# ========== MAKE PREDICTIONS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MAKING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "print(\"\\nFirst 10 predictions vs actual:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Probability_Class_1': y_pred_proba[:10]\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "# ========== MODEL EVALUATION - ACCURACY ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = log_reg.score(X_train_scaled, y_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# ========== CONFUSION MATRIX ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nTrue Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add text annotations\n",
    "plt.text(0.5, -0.15, f'TN = {tn}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(1.5, -0.15, f'FP = {fp}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(0.5, 1.15, f'FN = {fn}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "plt.text(1.5, 1.15, f'TP = {tp}', ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== CLASSIFICATION REPORT ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\", classification_report(y_test, y_pred, \n",
    "                                   target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# ========== ROC CURVE ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROC CURVE AND AUC SCORE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\nAUC (Area Under Curve) Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "         label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== COMBINED VISUALIZATION ==========\n",
    "print(\"\\nCreating combined visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Enhanced Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd', ax=axes[0],\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label', fontsize=11)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "# Plot 2: ROC Curve\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2.5, \n",
    "             label=f'Logistic Regression (AUC = {roc_auc:.4f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "             label='Random Guess (AUC = 0.5)')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.2, color='orange')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[1].set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(loc='lower right', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Logistic Regression Performance Metrics', \n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== ADDITIONAL METRICS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate additional metrics manually\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1_score:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal samples: {len(y)}\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Testing samples: {len(y_test)}\")\n",
    "print(f\"\\nModel Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if roc_auc > 0.9:\n",
    "    print(\"- Excellent model performance (AUC > 0.9)\")\n",
    "elif roc_auc > 0.8:\n",
    "    print(\"- Good model performance (AUC > 0.8)\")\n",
    "elif roc_auc > 0.7:\n",
    "    print(\"- Fair model performance (AUC > 0.7)\")\n",
    "else:\n",
    "    print(\"- Poor model performance (AUC < 0.7)\")\n",
    "\n",
    "print(f\"\\n- The model correctly classified {tp + tn} out of {len(y_test)} samples\")\n",
    "print(f\"- False predictions: {fp + fn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
